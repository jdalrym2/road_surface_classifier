{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Surface Classifier Dataset Preparation\n",
    "\n",
    "#### About the Dataset\n",
    "\n",
    "This dataset combines OSM ways w/ labeled surface type (i.e. asphalt, concrete, etc) with corresponding imagery using the [National Agriculture Imagery Program (NAIP) ArcGIS server](https://basemap.nationalmap.gov/arcgis/rest/services/USGSImageryOnly/MapServer). Additionally, binary masks for each of the ways are generated to highlight to our ML model which road in the image we intend to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting Data\n",
    "This dataset was derived from all [OpenStreetMap (OSM)](https://www.openstreetmap.org/) ways in the United States that have a labeled [`surface` tag](https://wiki.openstreetmap.org/wiki/Key:surface). The process to obtaining this data is generally as follows:\n",
    "- Download OSM data for the United States, [here](https://download.geofabrik.de/north-america/us.html)\n",
    "- Extract all OSM ways with labeled surface type using locally-hosted Overpass API\n",
    "  - I used [this Docker container by from wiktorn](https://hub.docker.com/r/wiktorn/overpass-api) and [run_overpass_api.sh](/run_overpass_api.sh) to host the OSM Overpass API locally without my downloaded data.\n",
    "  - The local Overpass API query can be performed using [perform_query.py](/perform_query.py).\n",
    "  - Use OGR to convert `.osm` (XML) format to `.gpkg` to be easier to work with. In this case we only need the ways (`lines`) layer:\n",
    "      ```bash\n",
    "      ogr2ogr us_w_road_surface.gpkg us_w_road_surface.osm lines\n",
    "      ```\n",
    "  - This `us_w_road_surface.gpkg` is the starting point for dataset preparation in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports / Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports needed everywhere\n",
    "import pathlib\n",
    "import traceback\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import PIL.Image, PIL.ImageDraw\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "# Tell GDAL & OGR to throw Python exceptions\n",
    "gdal.UseExceptions()\n",
    "ogr.UseExceptions()\n",
    "\n",
    "# Configuration:\n",
    "\n",
    "# Unfiltered GPKG path: this is the GPKG file of all ways in the US w/ a surface label\n",
    "# This was retrieved from the process above\n",
    "unfiltered_gpkg_path = pathlib.Path('/data/gis/us_road_surface/us_w_road_surface.gpkg')\n",
    "assert unfiltered_gpkg_path.is_file()\n",
    "\n",
    "# Path to NAIP on AWS S3 Object DB\n",
    "naip_aws_gpkg_path = pathlib.Path('/data/road_surface_classifier/naip_on_aws/naip_on_aws.gpkg')\n",
    "assert naip_aws_gpkg_path.is_file()\n",
    "\n",
    "# Output path: this is where all of the output data for the dataset will be dumped\n",
    "output_path = pathlib.Path('/data/road_surface_classifier/')\n",
    "output_path.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "# Train / Validation / Test Imagery percentages to use\n",
    "train_pc = 80\n",
    "val_pc = 20\n",
    "test_pc = 0\n",
    "assert train_pc + val_pc + test_pc == 100\n",
    "\n",
    "# Derive other filesystem paths we need:\n",
    "\n",
    "# Path to filtered GPKG file we will generate with only relevant\n",
    "# surface types that we will consider\n",
    "filtered_gpkg_path = output_path / 'us_w_road_surface_filtered.gpkg'\n",
    "# Path to OSM \"features\" CSV that includes basic metadata needed to fetch\n",
    "# imagery and correspond to labels / masks\n",
    "features_csv_path = output_path / 'features.csv'\n",
    "\n",
    "# Directory to hold all National Map imagery we download\n",
    "naip_db_path = output_path / 'imagery_xfer'\n",
    "naip_db_path.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "# Directory to hold all masks of OSM ways over NAIP Imagery\n",
    "naip_masks_path = output_path / 'masks'\n",
    "naip_masks_path.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "# Path to dataset reference files that we will use\n",
    "# These are just CSV files that specify basic metadata,\n",
    "# filesystem locations, and labels\n",
    "dataset_path = output_path / 'dataset_complex'\n",
    "dataset_path.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "# OSM surface tags we will consider\n",
    "# Change this to add / remove surface tags from dataset\n",
    "DEFAULT_SURFACE_TAGS = [\n",
    "    'concrete', 'asphalt', 'paving_stones', 'gravel', 'ground',\n",
    "    'concrete:plates', 'bricks', 'unpaved', 'dirt', 'compacted', 'paved'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering OSM Data\n",
    "This step loads our GPKG file, and filters the underlying data in two ways:\n",
    "  - Remove any ways with a surface tag that we do not wish to consider, such that all remaining ways have surface tags that are in `DEFAULT_SURFACE_TAGS`.\n",
    "  - Persist only the metadata we need for this dataset, and throw away anything else.\n",
    "\n",
    "After this step we effectively have a list of OSM ways w/ each of the following:\n",
    "  - OSM ID\n",
    "  - WKT LineString for the way\n",
    "  - `highway` tag (i.e. primary, secondary, etc)\n",
    "  - `surface` tag\n",
    "  \n",
    "The output is saved in a new GPKG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run this if we don't have us_w_road_surface_filtered.gpkg!\n",
    "\n",
    "# Variable to store feature data that we load\n",
    "feature_data = []\n",
    "\n",
    "# WGS84 Spatial Reference: all of OSM is EPSG:4326\n",
    "srs_wgs84 = osr.SpatialReference()\n",
    "srs_wgs84.ImportFromEPSG(4326)\n",
    "\n",
    "# Dataset is ogr OSM parsed file with \"lines\" layer exported\n",
    "ds = gdal.OpenEx(str(unfiltered_gpkg_path))\n",
    "layer = ds.GetLayer()\n",
    "feature_count = layer.GetFeatureCount()\n",
    "print('Loading & filtering dataset features...')\n",
    "for idx in tqdm(range(feature_count)):\n",
    "    # Get geometry, OSM ID, highway, and surface tag from each way\n",
    "    feature = layer.GetNextFeature()\n",
    "    highway = str(feature.GetField(2))\n",
    "    wkt_str = feature.GetGeometryRef().ExportToWkt()\n",
    "    osm_id = int(feature.GetField(0))\n",
    "    other_tags = str(feature.GetField(8))\n",
    "\n",
    "    # NOTE: parsing the misc. tags field is messy. This is about\n",
    "    # as good as it gets.\n",
    "    tags_dict = dict([[f.replace('\"', '') for f in e.split('\"=>\"')]\n",
    "                        for e in other_tags.split('\",\"')])\n",
    "    surface_type = tags_dict.get('surface', 'unknown')\n",
    "\n",
    "    # Get only the surface types we care about\n",
    "    if surface_type not in DEFAULT_SURFACE_TAGS:\n",
    "        continue\n",
    "\n",
    "    # Add to the feature data\n",
    "    feature_data.append([osm_id, wkt_str, highway, surface_type])\n",
    "\n",
    "# Close dataset\n",
    "layer = None\n",
    "ds = None\n",
    "\n",
    "# Start a new dataset\n",
    "print('Saving filtered features...')\n",
    "driver = ogr.GetDriverByName('GPKG')\n",
    "ds = driver.CreateDataSource(str(filtered_gpkg_path))\n",
    "layer = ds.CreateLayer('roads', srs=srs_wgs84, geom_type=ogr.wkbLineString)\n",
    "\n",
    "# Define fields\n",
    "id_field = ogr.FieldDefn('osmid', ogr.OFTInteger64)\n",
    "highway_field = ogr.FieldDefn('highway', ogr.OFTString)\n",
    "surface_field = ogr.FieldDefn('surface', ogr.OFTString)\n",
    "for field in (id_field, highway_field, surface_field):\n",
    "    layer.CreateField(field)\n",
    "\n",
    "# Add features\n",
    "feature_defn = layer.GetLayerDefn()\n",
    "for idx, (osm_id, wkt_str, highway,\n",
    "            surface_type) in tqdm(enumerate(feature_data)):\n",
    "\n",
    "    # New feature\n",
    "    feat = ogr.Feature(feature_defn)\n",
    "\n",
    "    # Set geometry\n",
    "    geom = ogr.CreateGeometryFromWkt(wkt_str)\n",
    "    feat.SetGeometry(geom)\n",
    "\n",
    "    # Set fields\n",
    "    feat.SetField('osmid', osm_id)\n",
    "    feat.SetField('highway', highway)\n",
    "    feat.SetField('surface', surface_type)\n",
    "\n",
    "    # Flush\n",
    "    layer.CreateFeature(feat)\n",
    "    feat = None\n",
    "\n",
    "# Close dataset\n",
    "layer = None\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Feature Data\n",
    "This step loads in our filtered feature data to prep it for use. Ultimately we are generating a list of tuples that will be converted to a Pandas `DataFrame` in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our filtered GPKG and extract all features to a list\n",
    "feature_data = []\n",
    "ds = gdal.OpenEx(str(filtered_gpkg_path))\n",
    "layer = ds.GetLayer()\n",
    "feature_count = layer.GetFeatureCount()\n",
    "print('Loading features from file...')\n",
    "for idx in tqdm(range(feature_count)):\n",
    "    feat = layer.GetNextFeature()\n",
    "    geom = feat.GetGeometryRef()\n",
    "    wkt = geom.ExportToWkt()\n",
    "    lon, lat = geom.GetPoint_2D(geom.GetPointCount() // 2)\n",
    "    osm_id = feat.GetField(0)\n",
    "    highway = feat.GetField(1)\n",
    "    surface = feat.GetField(2)\n",
    "    feature_data.append((osm_id, highway, surface, wkt, lon, lat))\n",
    "layer = None\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch NAIP Imagery\n",
    "\n",
    "Here we convert our feature data into a Pandas `DataFrame` and do a few things:\n",
    "  - Identify the slippy map tile coordinates we need to fetch from The National Map.\n",
    "  - Identify where in the tile (image coordinates) the way will appear.\n",
    "  - For now, don't consider any ways < 500 meters in length.\n",
    "  - Sort by the way's distance from the center of the tile. These are better quality samples.\n",
    "\n",
    "Next, for each surface type we limit ourselves to 5000 tiles, for now. This will help reduce class balance issues down the road. We have finalized our dataset.\n",
    "\n",
    "We then take our filtered and sorted data and fetch NAIP imagery for it, using a `NationalMapFetcherNAIP` object. This will result in a filesystem database of tiles that can easily be referenced for future ML training.\n",
    "\n",
    "At this point we save all of our accrued data into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in helper functions\n",
    "from data_prep.utils import wgs84_to_xy, get_length_m\n",
    "\n",
    "# Load feature data list from earlier and set the column names\n",
    "df = pd.DataFrame(\n",
    "    feature_data,\n",
    "    columns=['osm_id', 'highway', 'surface', 'wkt', 'lon', 'lat'])\n",
    "\n",
    "# Load NAIP on AWS data\n",
    "ds: ogr.DataSource = ogr.Open(str(naip_aws_gpkg_path))\n",
    "layer: ogr.Layer = ds.GetLayer()\n",
    "\n",
    "# Next, find the map tile and image x, y files for each point in the dataframe\n",
    "df['x'], df['y'], df['ix'], df['iy'] = wgs84_to_xy(\n",
    "    df['lon'].to_numpy(), df['lat'].to_numpy(), 17, iw = 256, ih = 256)\n",
    "df['x'] = df['x'] + (df['ix'] - 128) / 256\n",
    "df['y'] = df['y'] + (df['iy'] - 128) / 256\n",
    "df['ix'] = 128\n",
    "df['iy'] = 128\n",
    "\n",
    "# Calculate length of each feature, and limit features to >10m\n",
    "# This will allow us to avoid small things like roundabouts or other short ways\n",
    "print('Computing way lengths and filtering...')\n",
    "df['length'] = [get_length_m(wkt_str) for wkt_str in tqdm(df['wkt'])]\n",
    "df = df[df['length'] >= 10]\n",
    "\n",
    "# Shuffle!\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# For each unique surface type we have, we will filter the dataframe\n",
    "# to get the first 5000 entries if there are more than this. This will\n",
    "# make class weighting a little easier down the road.\n",
    "series = []\n",
    "for surface in np.unique(df['surface']):\n",
    "    sub_df = df[df['surface'] == surface]\n",
    "    sub_df = sub_df[0:min(5000, len(sub_df))]\n",
    "    series.append(sub_df)\n",
    "df_filtered = pd.concat(series)\n",
    "\n",
    "# Cross reference\n",
    "def cross_ref_naip(row):\n",
    "    # Create point geometry to apply\n",
    "    geom = ogr.Geometry(ogr.wkbPoint)\n",
    "    geom.AddPoint_2D(row['lon'], row['lat'])\n",
    "\n",
    "    # Apply filter\n",
    "    layer.SetSpatialFilter(geom)\n",
    "\n",
    "    # Get features that fit the filter\n",
    "    feat_data = []\n",
    "    for _ in range(layer.GetFeatureCount()):\n",
    "        feat: ogr.Feature = layer.GetNextFeature()\n",
    "        feat_date = int(feat.GetFieldAsString('SRCIMGDATE'))\n",
    "        feat_obj = feat.GetFieldAsString('OBJECT')\n",
    "        feat_data.append((feat_date, feat_obj))\n",
    "\n",
    "    # If nothing found, return None\n",
    "    if not len(feat_data):\n",
    "        return None\n",
    "    \n",
    "    # Sort by date and return object\n",
    "    feat_data.sort(key=lambda pair: pair[0])\n",
    "    _, obj = feat_data[-1]\n",
    "    return obj\n",
    "    \n",
    "print('Cross referencing with NAIP on AWS...')\n",
    "\n",
    "df_filtered['object'] = df_filtered.apply(cross_ref_naip, axis=1)\n",
    "\n",
    "df_filtered = df_filtered[df_filtered['object'].notnull()]\n",
    "\n",
    "# Close out NAIP on AWS dataset\n",
    "layer = None\n",
    "ds = None\n",
    "\n",
    "# Save completed features to CSV\n",
    "df_filtered.to_csv(str(features_csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from multiprocessing import Pool\n",
    "from data_prep.aws_naip import get_naip_file, AWS_PATH\n",
    "from data_prep.utils import map_to_pix, imread, imread_geotransform, imread_srs, imwrite\n",
    "\n",
    "def get_naip_analytic(obj: str) -> pathlib.Path:\n",
    "    obj_path = pathlib.PosixPath(obj)\n",
    "    out_path = get_naip_file(str(obj_path.with_suffix('.mrf')), bucket_name='naip-analytic')\n",
    "    for ext in ('.idx', '.lrc', '.mrf.aux.xml'):\n",
    "        get_naip_file(str(obj_path.with_suffix(ext)), bucket_name='naip-analytic')\n",
    "    return out_path\n",
    "\n",
    "def unlink_naip_analytic(obj: str):\n",
    "    obj_path = AWS_PATH / pathlib.PosixPath(obj)\n",
    "    for f in obj_path.parent.glob(f'{obj_path.stem}*'):\n",
    "        f.unlink()\n",
    "\n",
    "def chip_image(input_path, output_path, x, y, w, h):\n",
    "    \"\"\" Helper function to chip an image \"\"\"\n",
    "    im = imread(str(input_path), x, y, w, h)\n",
    "    xform = imread_geotransform(str(input_path), x, y)\n",
    "    srs = imread_srs(str(input_path))\n",
    "    imwrite(im, str(output_path), xform=xform, srs=srs)\n",
    "\n",
    "# Prep to download NAIP Imagery\n",
    "df = pd.read_csv(str(features_csv_path))\n",
    "df_obj_unique = df['object'].unique()\n",
    "\n",
    "# Get 4326 SRS to use for coordinate transformations\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4326)\n",
    "srs.SetAxisMappingStrategy(osr.OAMS_TRADITIONAL_GIS_ORDER)\n",
    "\n",
    "def process_naip_obj(this_tup):\n",
    "    \"\"\" Helper function to chip out a NAIP image from our dataframe \"\"\"\n",
    "    idx, total, this_obj = this_tup\n",
    "    print('Processing %d of %d...' % (idx, total), flush=True)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Filter dataframe for chips with this NAIP ID\n",
    "        df_obj = df[df['object'] == this_obj]\n",
    "\n",
    "        # Lazy init image\n",
    "        naip_path = None\n",
    "\n",
    "        for row_idx in range(len(df_obj)):\n",
    "            try:\n",
    "                # Read row\n",
    "                this_row = df_obj.iloc[row_idx]\n",
    "                osm_id = this_row['osm_id']\n",
    "                lon, lat = this_row['lon'], this_row['lat']\n",
    "\n",
    "                # Compute chip path: skip if exists\n",
    "                chip_path = naip_db_path / f'{osm_id}.tif'\n",
    "                if chip_path.exists():\n",
    "                    print('Skipping: %s' % str(chip_path), flush=True)\n",
    "                    continue\n",
    "\n",
    "                # Download the image if not fetched yet\n",
    "                if naip_path is None:\n",
    "                    print('Downloading %s...' % this_obj, flush=True)\n",
    "                    naip_path = get_naip_analytic(this_obj)\n",
    "\n",
    "                # Compute coordinate transformation\n",
    "                ds: gdal.Dataset = gdal.Open(str(naip_path), gdal.GA_ReadOnly)\n",
    "                g_xform = ds.GetGeoTransform()\n",
    "                srs_ds: osr.SpatialReference = ds.GetSpatialRef()\n",
    "                srs_ds.SetAxisMappingStrategy(osr.OAMS_TRADITIONAL_GIS_ORDER)\n",
    "                ds = None  # type: ignore\n",
    "                c_xform = osr.CoordinateTransformation(srs, srs_ds)\n",
    "\n",
    "                # Compute center point in image spatial coordinate system\n",
    "                pt = ogr.Geometry(ogr.wkbPoint)\n",
    "                pt.AddPoint_2D(lon, lat)\n",
    "                pt.Transform(c_xform)\n",
    "\n",
    "                # Compute upper-left corner for image chip\n",
    "                x, y = [\n",
    "                    e[0].item()\n",
    "                    for e in map_to_pix(g_xform, pt.GetX(), pt.GetY())\n",
    "                ]\n",
    "                x1, y1 = x - 128, y - 128\n",
    "\n",
    "                # Chip the image!\n",
    "                chip_image(naip_path, chip_path, x1, y1, 256, 256)\n",
    "                print('Done with: %s!' % str(naip_path), flush=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                print('Exception occurred processing chip. Continuing.',\n",
    "                      flush=True)\n",
    "\n",
    "        # Unlink if we downloaded the image\n",
    "        if naip_path is not None:\n",
    "            unlink_naip_analytic(this_obj)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print('Exception occurred processing NAIP image. Continuing.',\n",
    "              flush=True)\n",
    "\n",
    "# Perform the download!\n",
    "USE_MULTIPROCESSING = True\n",
    "if USE_MULTIPROCESSING:\n",
    "    with Pool(2) as p:\n",
    "        p.map(process_naip_obj,\n",
    "                       [(idx, len(df_obj_unique), df_obj_unique[idx])\n",
    "                        for idx in range(len(df_obj_unique))],\n",
    "                       chunksize=1)\n",
    "else:\n",
    "    for idx in range(len(df_obj_unique)):\n",
    "        process_naip_obj((idx, len(df_obj_unique), df_obj_unique[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Binary Masks\n",
    "\n",
    "This step will, for every OSM way in our dataset, create a binary mask that matches the NAIP tile for that way. This will be our way of showing the model \"classify this road, here\" in the event there are multiple roads in the tile, or if the road is not easily visible. In general, any helpful metadata we can provide an ML model (in the correct way), the better.\n",
    "\n",
    "These masks will be saved in yet another filesystem database where they can be easily referenced later by OSM ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating masks from OSM data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46526/46526 [03:55<00:00, 197.22it/s] \n"
     ]
    }
   ],
   "source": [
    "from data_prep.utils import wgs84_to_im\n",
    "\n",
    "from data_prep.utils import map_to_pix, imread, imread_geotransform, imread_srs\n",
    "\n",
    "# Load features dataframe\n",
    "df = pd.read_csv(str(features_csv_path))\n",
    "\n",
    "# Standard WGS-84 spatial reference\n",
    "srs_ref = osr.SpatialReference()\n",
    "srs_ref.ImportFromEPSG(4326)\n",
    "srs_ref.SetAxisMappingStrategy(osr.OAMS_TRADITIONAL_GIS_ORDER)\n",
    "\n",
    "# Mask creation loop\n",
    "print('Creating masks from OSM data...')\n",
    "for idx in tqdm(range(len(df))):\n",
    "\n",
    "    # Load in row\n",
    "    row = df.iloc[idx]\n",
    "\n",
    "    # Chip path\n",
    "    chip_path = naip_db_path / f'{row[\"osm_id\"]}.tif'\n",
    "    if not chip_path.exists():\n",
    "        continue\n",
    "\n",
    "    # Fetch the tile we need\n",
    "    im = imread(str(chip_path))\n",
    "    xform = imread_geotransform(str(chip_path))\n",
    "\n",
    "    # Get the spatial reference (they change across the tiles)\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromProj4(imread_srs(str(chip_path)))\n",
    "    srs.SetAxisMappingStrategy(osr.OAMS_TRADITIONAL_GIS_ORDER)\n",
    "    im_trans = osr.CoordinateTransformation(srs_ref, srs)\n",
    "\n",
    "    # Get list of points from WKT\n",
    "    geom: ogr.Geometry = ogr.CreateGeometryFromWkt(row['wkt'])\n",
    "    geom.Transform(im_trans)\n",
    "    pts = np.array(\n",
    "        [geom.GetPoint_2D(idx) for idx in range(geom.GetPointCount())])\n",
    "\n",
    "    # Convert to image-space x, y\n",
    "    ix, iy = map_to_pix(xform, pts[:, 0], pts[:, 1])\n",
    "\n",
    "    # Create a new image of the same shape, and draw a line\n",
    "    # to create a mask\n",
    "    mask = np.zeros((im.shape[0], im.shape[1]), dtype=np.uint8)\n",
    "    mask_pil = PIL.Image.new('L', ((im.shape[1], im.shape[0])), color=0)\n",
    "    d = PIL.ImageDraw.Draw(mask_pil)\n",
    "    d.line(\n",
    "        [(x, y) for x, y in zip(ix, iy)],     # type: ignore\n",
    "        fill=255,\n",
    "        width=20, # Original: 10,\n",
    "        joint=\"curve\")\n",
    "    mask = np.array(mask_pil)\n",
    "\n",
    "    # Save the mask\n",
    "    out_file = naip_masks_path / ('%d.png' % row['osm_id'])\n",
    "    mask_pil.save(out_file, format='PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Prep for Training\n",
    "This step takes care of the rest of the legwork needed to ingest this data into our model. Here's the general process:\n",
    "  - Create a mapping from OSM surface classes to classes our model will learn. For now, I'm going simple and attempting to learn \"paved\" vs. \"unpaved\".\n",
    "  - For each OSM way, fetch the path to its corresponding NAIP image and binary mask.\n",
    "  - Split the data into train / validation sets, and optionally a test set too.\n",
    "  - Compute class weights to use to help with a somewhat unbalanced resulting dataset.\n",
    "  - Save off everything into a CSV file to be later used by the data loaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features dataframe\n",
    "df = pd.read_csv(str(features_csv_path))\n",
    "\n",
    "# Mapping of OSM classes to classes to be learned by\n",
    "# our model\n",
    "# class_map = {\n",
    "#     'asphalt': 'paved',\n",
    "#     'bricks': 'paved',\n",
    "#     'compacted': 'unpaved',\n",
    "#     'concrete': 'paved',\n",
    "#     'concrete:plates': 'paved',\n",
    "#     'dirt': 'unpaved',\n",
    "#     'gravel': 'unpaved',\n",
    "#     'ground': 'unpaved',\n",
    "#     'paved': 'paved',\n",
    "#     'paving_stones': 'paved',\n",
    "#     'unpaved': 'unpaved',\n",
    "# }\n",
    "class_map = {\n",
    "    'asphalt': 'asphalt',\n",
    "    'bricks': 'bricks',\n",
    "    'compacted': 'unpaved',\n",
    "    'concrete': 'concrete',\n",
    "    'concrete:plates': 'concrete',\n",
    "    'dirt': 'unpaved',\n",
    "    'gravel': 'unpaved',\n",
    "    'ground': 'unpaved',\n",
    "    #'paved': 'paved',\n",
    "    #'paving_stones': 'paved',\n",
    "    'unpaved': 'unpaved',\n",
    "}\n",
    "df = df.iloc[[idx for idx, e in enumerate(df['surface']) if e in class_map]]\n",
    "df_surface = [class_map[e] for e in df['surface']]\n",
    "\n",
    "# Get set of unique classes and number them\n",
    "classes = np.unique(df_surface)\n",
    "classes_n = np.arange(len(classes))\n",
    "classes_map = {k: v for k, v in zip(classes, classes_n)}\n",
    "\n",
    "# Get dataset chip paths\n",
    "print('Getting chip paths...')\n",
    "chip_paths = [\n",
    "    pathlib.Path(naip_db_path, '17', '%d' % y, '%d.jpg' % x).resolve()\n",
    "    for x, y in zip(df['x'], df['y'])\n",
    "]\n",
    "try:\n",
    "    assert all([e.exists() for e in chip_paths]), 'Some chip paths do not exist!'\n",
    "except AssertionError:\n",
    "    print([e for e in chip_paths if not e.exists()])\n",
    "    raise\n",
    "\n",
    "# Get dataset mask paths\n",
    "print('Getting mask paths...')\n",
    "mask_paths = [\n",
    "    pathlib.Path(naip_masks_path, '%d.png' % osm_id).resolve()\n",
    "    for osm_id in df['osm_id']\n",
    "]\n",
    "assert all([e.exists() for e in mask_paths])\n",
    "\n",
    "# Create new dataframe from CSV\n",
    "# OSMID, Class Num, Chip Path, Mask Path\n",
    "df_full = pd.DataFrame(\n",
    "    dict(osm_id=df['osm_id'],\n",
    "            class_num=[classes_map[e] for e in df_surface],\n",
    "            chip_path=[str(e) for e in chip_paths],\n",
    "            mask_path=[str(e) for e in mask_paths]))\n",
    "\n",
    "# Use size of dataset to determine number of\n",
    "# train / val / test features to use\n",
    "num_train = np.round(len(df_full) * train_pc / 100).astype(int).item()\n",
    "num_val = np.round(len(df_full) * val_pc / 100).astype(int).item()\n",
    "num_test = len(df_full) - num_train - num_val\n",
    "\n",
    "# Create a list of all row idxs and shuffle!\n",
    "idxs = np.arange(len(df_full))\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "# Get list of row idxs for train / val / test\n",
    "idxs_train = idxs[:num_train]\n",
    "idxs_val = idxs[num_train:(num_train + num_val)]\n",
    "idxs_test = idxs[(num_train + num_val):(num_train + num_val + num_test)]\n",
    "\n",
    "# Split dataframe into train / val / test\n",
    "df_train = df_full.iloc[idxs_train]\n",
    "df_val = df_full.iloc[idxs_val]\n",
    "df_test = df_full.iloc[idxs_test]\n",
    "\n",
    "# Class Name, Class Num, Total Train, Total Val, Total Test, Total, Weight\n",
    "class_total_full = [sum(df_full['class_num'] == n) for n in classes_n]\n",
    "class_total_train = [sum(df_train['class_num'] == n) for n in classes_n]\n",
    "class_total_val = [sum(df_val['class_num'] == n) for n in classes_n]\n",
    "class_total_test = [sum(df_test['class_num'] == n) for n in classes_n]\n",
    "\n",
    "# Define class weights\n",
    "class_weights = [(len(df_full) / len(classes)) / (e)\n",
    "                    for e in class_total_full]\n",
    "\n",
    "df_class = pd.DataFrame(\n",
    "    dict(class_name=classes,\n",
    "            class_num=classes_n,\n",
    "            total_train=class_total_train,\n",
    "            total_val=class_total_val,\n",
    "            total_test=class_total_test,\n",
    "            total=class_total_full,\n",
    "            weight=class_weights))\n",
    "\n",
    "# Save dataframes\n",
    "print('Saving dataframes...')\n",
    "df_full.to_csv(pathlib.Path(dataset_path, 'dataset_full.csv'))\n",
    "df_train.to_csv(pathlib.Path(dataset_path, 'dataset_train.csv'))\n",
    "df_val.to_csv(pathlib.Path(dataset_path, 'dataset_val.csv'))\n",
    "df_test.to_csv(pathlib.Path(dataset_path, 'dataset_test.csv'))\n",
    "df_class.to_csv(pathlib.Path(dataset_path, 'class_weights.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset is Complete!\n",
    "\n",
    "Well, that was a lot of work and computation time. Let's take some time to look at some pretty NAIP pictures and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_cols = 6\n",
    "\n",
    "df_full = pd.read_csv(pathlib.Path(dataset_path, 'dataset_full.csv'))\n",
    "df_class = pd.read_csv(pathlib.Path(dataset_path, 'class_weights.csv'))\n",
    "\n",
    "random_idxs = np.random.choice(len(df_full), size=n_cols, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(2, n_cols, figsize=(3*n_cols, 6))\n",
    "for i, idx in enumerate(random_idxs):\n",
    "    row = df_full.iloc[idx]\n",
    "\n",
    "    with PIL.Image.open(row['chip_path']) as pil_im:\n",
    "        ax[0, i].imshow(pil_im)\n",
    "        ax[0, i].set_title('ID: %d \\n Label: %s' % (row['osm_id'], df_class.iloc[row['class_num']]['class_name']))\n",
    "    \n",
    "    with PIL.Image.open(row['mask_path']) as pil_im:\n",
    "        ax[1, i].imshow(pil_im)\n",
    "\n",
    "for _ax in ax.flatten():\n",
    "    _ax.xaxis.set_visible(False)\n",
    "    _ax.yaxis.set_visible(False)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d102384ded633c24f1031e288c2ecf1ababc4ef37e402995ad37064232eefd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
